{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9db92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ç¡¬ä»¶åŠ é€Ÿå·²é”å®š: mps (Apple M4)\n",
      "â³ æ­£åœ¨åŠ è½½ DistilBERT Tokenizer...\n",
      "âœ… æ•°æ®è£…è½½å®Œæ¯•ï¼è®­ç»ƒé›†: 175428, æµ‹è¯•é›†: 43858\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å‡†å¤‡ç¯å¢ƒä¸æ•°æ®\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "\n",
    "# 1. é”å®š M4 çš„ MPS åŠ é€Ÿ\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"ğŸ”¥ ç¡¬ä»¶åŠ é€Ÿå·²é”å®š: {device} (Apple M4)\")\n",
    "\n",
    "# 2. è¯»å–æ•°æ®\n",
    "filename = 'processed_data.csv'\n",
    "if os.path.exists(os.path.join('..', 'data', filename)):\n",
    "    df = pd.read_csv(os.path.join('..', 'data', filename))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join('data', filename))\n",
    "\n",
    "# 3. æ ‡ç­¾å¤„ç†\n",
    "df = df.dropna(subset=['cleaned_text', 'labels'])\n",
    "label_map = {'bad': 0, 'neutral': 1, 'good': 2}\n",
    "df['label_id'] = df['labels'].map(label_map)\n",
    "\n",
    "# 4. åŠ è½½ Tokenizer (DistilBERT)\n",
    "# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½å­—å…¸ï¼Œçº¦ 200MB\n",
    "print(\"â³ æ­£åœ¨åŠ è½½ DistilBERT Tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')#ä¸åŒºåˆ†å¤§å°å†™uncased\n",
    "\n",
    "# 5. å®šä¹‰ Dataset ç±» (æ ¸å¿ƒ)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # æ ¸å¿ƒï¼šä½¿ç”¨ BERT çš„ tokenizer ç¼–ç \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 6. åˆ’åˆ†æ•°æ®\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label_id'], test_size=0.2, random_state=42)\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "# batch_size=32 æ˜¯ M4 16GB å†…å­˜çš„å®‰å…¨å€¼ï¼Œå¦‚æœæŠ¥é”™å†…å­˜ä¸è¶³ï¼Œæ”¹ 16\n",
    "train_dataset = SentimentDataset(X_train.to_numpy(), y_train.to_numpy(), tokenizer)\n",
    "test_dataset = SentimentDataset(X_test.to_numpy(), y_test.to_numpy(), tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"âœ… æ•°æ®è£…è½½å®Œæ¯•ï¼è®­ç»ƒé›†: {len(train_dataset)}, æµ‹è¯•é›†: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bef8ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä» model_save åŠ è½½å¾®è°ƒåçš„ DistilBERT æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model_path = \"./model_save\"  # ä½ ä¹‹å‰ä¿å­˜æ¨¡å‹çš„ç›®å½•\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "print(\"âœ… å·²ä» model_save åŠ è½½å¾®è°ƒåçš„ DistilBERT æ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ca4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ (DistilBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒï¼è®¡åˆ’è½®æ•°: 1\n",
      "ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥æ‰“å¼€ Activity Monitor è§‚å¯Ÿ GPU å ç”¨ç‡\n",
      "Epoch 1 | Batch 100/5483 | Loss: 0.6954 | Acc: 0.5724\n",
      "Epoch 1 | Batch 200/5483 | Loss: 0.5719 | Acc: 0.6502\n",
      "Epoch 1 | Batch 300/5483 | Loss: 0.5356 | Acc: 0.6894\n",
      "Epoch 1 | Batch 400/5483 | Loss: 0.6521 | Acc: 0.7126\n",
      "Epoch 1 | Batch 500/5483 | Loss: 0.6119 | Acc: 0.7297\n",
      "Epoch 1 | Batch 600/5483 | Loss: 0.4354 | Acc: 0.7434\n",
      "Epoch 1 | Batch 700/5483 | Loss: 0.5078 | Acc: 0.7554\n",
      "Epoch 1 | Batch 800/5483 | Loss: 0.4188 | Acc: 0.7654\n",
      "Epoch 1 | Batch 900/5483 | Loss: 0.6083 | Acc: 0.7742\n",
      "Epoch 1 | Batch 1000/5483 | Loss: 0.5006 | Acc: 0.7807\n",
      "Epoch 1 | Batch 1100/5483 | Loss: 0.4335 | Acc: 0.7874\n",
      "Epoch 1 | Batch 1200/5483 | Loss: 0.2063 | Acc: 0.7946\n",
      "Epoch 1 | Batch 1300/5483 | Loss: 0.4786 | Acc: 0.8005\n",
      "Epoch 1 | Batch 1400/5483 | Loss: 0.2565 | Acc: 0.8061\n",
      "Epoch 1 | Batch 1500/5483 | Loss: 0.3125 | Acc: 0.8101\n",
      "Epoch 1 | Batch 1600/5483 | Loss: 0.3931 | Acc: 0.8142\n",
      "Epoch 1 | Batch 1700/5483 | Loss: 0.1843 | Acc: 0.8180\n",
      "Epoch 1 | Batch 1800/5483 | Loss: 0.1592 | Acc: 0.8224\n",
      "Epoch 1 | Batch 1900/5483 | Loss: 0.2244 | Acc: 0.8249\n",
      "Epoch 1 | Batch 2000/5483 | Loss: 0.3774 | Acc: 0.8285\n",
      "Epoch 1 | Batch 2100/5483 | Loss: 0.1542 | Acc: 0.8316\n",
      "Epoch 1 | Batch 2200/5483 | Loss: 0.2495 | Acc: 0.8345\n",
      "Epoch 1 | Batch 2300/5483 | Loss: 0.2893 | Acc: 0.8373\n",
      "Epoch 1 | Batch 2400/5483 | Loss: 0.1146 | Acc: 0.8400\n",
      "Epoch 1 | Batch 2500/5483 | Loss: 0.4530 | Acc: 0.8421\n",
      "Epoch 1 | Batch 2600/5483 | Loss: 0.2220 | Acc: 0.8443\n",
      "Epoch 1 | Batch 2700/5483 | Loss: 0.2009 | Acc: 0.8469\n",
      "Epoch 1 | Batch 2800/5483 | Loss: 0.2730 | Acc: 0.8490\n",
      "Epoch 1 | Batch 2900/5483 | Loss: 0.1899 | Acc: 0.8508\n",
      "Epoch 1 | Batch 3000/5483 | Loss: 0.2104 | Acc: 0.8523\n",
      "Epoch 1 | Batch 3100/5483 | Loss: 0.2807 | Acc: 0.8538\n",
      "Epoch 1 | Batch 3200/5483 | Loss: 0.2416 | Acc: 0.8554\n",
      "Epoch 1 | Batch 3300/5483 | Loss: 0.3990 | Acc: 0.8571\n",
      "Epoch 1 | Batch 3400/5483 | Loss: 0.0757 | Acc: 0.8587\n",
      "Epoch 1 | Batch 3500/5483 | Loss: 0.1317 | Acc: 0.8601\n",
      "Epoch 1 | Batch 3600/5483 | Loss: 0.2405 | Acc: 0.8613\n",
      "Epoch 1 | Batch 3700/5483 | Loss: 0.3511 | Acc: 0.8627\n",
      "Epoch 1 | Batch 3800/5483 | Loss: 0.2476 | Acc: 0.8641\n",
      "Epoch 1 | Batch 3900/5483 | Loss: 0.1329 | Acc: 0.8654\n",
      "Epoch 1 | Batch 4000/5483 | Loss: 0.1192 | Acc: 0.8665\n",
      "Epoch 1 | Batch 4100/5483 | Loss: 0.3509 | Acc: 0.8676\n",
      "Epoch 1 | Batch 4200/5483 | Loss: 0.3185 | Acc: 0.8686\n",
      "Epoch 1 | Batch 4300/5483 | Loss: 0.3229 | Acc: 0.8698\n",
      "Epoch 1 | Batch 4400/5483 | Loss: 0.0906 | Acc: 0.8711\n",
      "Epoch 1 | Batch 4500/5483 | Loss: 0.3591 | Acc: 0.8720\n",
      "Epoch 1 | Batch 4600/5483 | Loss: 0.2378 | Acc: 0.8729\n",
      "Epoch 1 | Batch 4700/5483 | Loss: 0.1777 | Acc: 0.8739\n",
      "Epoch 1 | Batch 4800/5483 | Loss: 0.3440 | Acc: 0.8749\n",
      "Epoch 1 | Batch 4900/5483 | Loss: 0.5766 | Acc: 0.8757\n",
      "Epoch 1 | Batch 5000/5483 | Loss: 0.1499 | Acc: 0.8765\n",
      "Epoch 1 | Batch 5100/5483 | Loss: 0.3214 | Acc: 0.8772\n",
      "Epoch 1 | Batch 5200/5483 | Loss: 0.0983 | Acc: 0.8782\n",
      "Epoch 1 | Batch 5300/5483 | Loss: 0.1250 | Acc: 0.8789\n",
      "Epoch 1 | Batch 5400/5483 | Loss: 0.0837 | Acc: 0.8798\n",
      "ğŸ† Epoch 1 å®Œæˆ! Average Loss: 0.3139 | Average Acc: 0.8804\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (ä¿®æ­£ç‰ˆ): è®­ç»ƒ DistilBERT (é€‚é… Mac MPS float32)\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "print(\"â³ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ (DistilBERT)...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=3\n",
    ")\n",
    "model = model.to(device) # æ¬åˆ° GPU\n",
    "\n",
    "# ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# === è®­ç»ƒå¼€å§‹ ===\n",
    "EPOCHS = 1 \n",
    "print(f\"ğŸš€ å¼€å§‹è®­ç»ƒï¼è®¡åˆ’è½®æ•°: {EPOCHS}\")\n",
    "print(\"ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥æ‰“å¼€ Activity Monitor è§‚å¯Ÿ GPU å ç”¨ç‡\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    n_examples = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # âš ï¸âš ï¸âš ï¸ ä¿®æ­£ç‚¹ï¼šä½¿ç”¨ .float() ä»£æ›¿ .double()\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        n_examples += labels.size(0)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            # è¿™é‡Œçš„è®¡ç®—é€»è¾‘ä¹Ÿæ”¹ä¸ºäº† float\n",
    "            curr_acc = correct_predictions.float() / n_examples\n",
    "            print(f\"Epoch {epoch+1} | Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {curr_acc.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    # âš ï¸âš ï¸âš ï¸ ä¿®æ­£ç‚¹ï¼šä½¿ç”¨ .float()\n",
    "    avg_acc = correct_predictions.float() / n_examples\n",
    "    print(f\"ğŸ† Epoch {epoch+1} å®Œæˆ! Average Loss: {avg_loss:.4f} | Average Acc: {avg_acc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bdef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨è¯„ä¼°æµ‹è¯•é›†...\n",
      "\n",
      "ğŸ¤– DistilBERT æœ€ç»ˆæˆ˜æŠ¥:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.97      0.95      0.96     21504\n",
      "     neutral       0.86      0.89      0.87     11075\n",
      "        good       0.93      0.93      0.93     11279\n",
      "\n",
      "    accuracy                           0.93     43858\n",
      "   macro avg       0.92      0.92      0.92     43858\n",
      "weighted avg       0.93      0.93      0.93     43858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: æœ€ç»ˆè¯„ä¼°\n",
    "print(\"â³ æ­£åœ¨è¯„ä¼°æµ‹è¯•é›†...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "real_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        real_values.extend(labels.cpu().numpy())\n",
    "\n",
    "target_names = ['bad', 'neutral', 'good']\n",
    "print(\"\\nğŸ¤– DistilBERT æœ€ç»ˆæˆ˜æŠ¥:\")\n",
    "print(classification_report(real_values, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41cacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ° ./model_save/ ...\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜ï¼æ–‡ä»¶å¤¹é‡Œåº”è¯¥æœ‰ 'config.json', 'pytorch_model.bin' ç­‰æ–‡ä»¶ã€‚\n",
      "ğŸ‘‰ é˜Ÿå‹å¦‚æœè¦ç”¨ï¼Œç›´æ¥ç”¨ DistilBertForSequenceClassification.from_pretrained('./model_save/') å³å¯åŠ è½½ã€‚\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: ä¿å­˜æ¨¡å‹ä¸ Tokenizer\n",
    "import os\n",
    "\n",
    "# åˆ›å»ºä¿å­˜ç›®å½•\n",
    "output_dir = './model_save/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ° {output_dir} ...\")\n",
    "\n",
    "# å…³é”®ï¼šæˆ‘ä»¬è¦ä¿å­˜æ¨¡å‹æœ¬èº«å’Œå¯¹åº”çš„åˆ†è¯å™¨\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # å¤„ç†å¤šå¡æƒ…å†µ(è™½åœ¨ä½ è¿™ä¹Ÿæ˜¯å•å¡)\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ… æ¨¡å‹å·²ä¿å­˜ï¼æ–‡ä»¶å¤¹é‡Œåº”è¯¥æœ‰ 'config.json', 'pytorch_model.bin' ç­‰æ–‡ä»¶ã€‚\")\n",
    "print(\"ğŸ‘‰ é˜Ÿå‹å¦‚æœè¦ç”¨ï¼Œç›´æ¥ç”¨ DistilBertForSequenceClassification.from_pretrained('./model_save/') å³å¯åŠ è½½ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25269cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° bert_eval_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:  # âœ… ç”¨ test_loader åšè¯„ä¼°\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_confidences.extend(conf.cpu().numpy())\n",
    "\n",
    "df_eval = pd.DataFrame({\n",
    "    \"label\": all_labels,\n",
    "    \"pred\": all_preds,\n",
    "    \"confidence\": all_confidences\n",
    "})\n",
    "\n",
    "df_eval.to_csv(\"bert_eval_predictions.csv\", index=False)\n",
    "print(\"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° bert_eval_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2af7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cbacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_senti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
