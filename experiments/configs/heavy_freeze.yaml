# 重度冻结实验 - 只训练分类器
experiment:
  name: "freeze_all_transformers"
  description: "Freeze all transformer layers, only train classifier head"
  save_model: false

model:
  name: "distilbert-base-uncased"
  num_labels: 3
  freeze_layers: 6  # 冻结全部6层transformer

training:
  learning_rate: 1.0e-4  # 可以用更高的学习率
  batch_size: 64  # 可以用更大的batch
  num_epochs: 1
  weight_decay: 0.01
  warmup_steps: 200
  max_grad_norm: 1.0

optimizer:
  type: "AdamW"
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: null

data:
  max_seq_length: 128
  test_size: 0.2
  random_seed: 42

output:
  save_confusion_matrix: true
  save_classification_report: true
  save_training_curves: true
  save_predictions: false

notes: |
  只训练分类器头的实验：
  - 训练速度最快
  - 参数量最少
  - 适合作为快速baseline
