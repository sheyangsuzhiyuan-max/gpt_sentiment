data:
  max_seq_length: 128
  random_seed: 42
  test_size: 0.2
experiment:
  description: Lower learning rate (1e-5) for more stable training
  name: lower_lr_1e-5
  save_model: false
model:
  freeze_layers: 0
  name: distilbert-base-uncased
  num_labels: 3
optimizer:
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  type: AdamW
output:
  save_classification_report: true
  save_confusion_matrix: true
  save_predictions: false
  save_training_curves: true
scheduler:
  type: linear
  warmup_ratio: 0.1
training:
  batch_size: 32
  learning_rate: 1.0e-05
  max_grad_norm: 1.0
  num_epochs: 1
  warmup_steps: 500
  weight_decay: 0.01
