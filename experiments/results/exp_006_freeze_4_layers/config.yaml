data:
  max_seq_length: 128
  random_seed: 42
  test_size: 0.2
experiment:
  description: Freeze first 4 transformer layers, fine-tune top layers only
  name: freeze_4_layers
  save_model: false
model:
  freeze_layers: 4
  name: distilbert-base-uncased
  num_labels: 3
notes: "\u5B9E\u9A8C\u5047\u8BBE\uFF1A\n1. \u51BB\u7ED3\u5E95\u5C42\u53EF\u4EE5\u4FDD\
  \u7559\u9884\u8BAD\u7EC3\u77E5\u8BC6\n2. \u53EA\u5FAE\u8C03\u9876\u5C42\u53EF\u4EE5\
  \u52A0\u5FEB\u8BAD\u7EC3\n3. \u66F4\u9AD8\u7684\u5B66\u4E60\u7387\u4E0D\u4F1A\u5BFC\
  \u81F4\u5E95\u5C42\u77E5\u8BC6\u635F\u5931\n"
optimizer:
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  type: AdamW
output:
  save_classification_report: true
  save_confusion_matrix: true
  save_predictions: false
  save_training_curves: true
scheduler:
  type: linear
  warmup_ratio: 0.1
training:
  batch_size: 32
  learning_rate: 5.0e-05
  max_grad_norm: 1.0
  num_epochs: 1
  warmup_steps: 500
  weight_decay: 0.01
