"""
æƒ…æ„Ÿåˆ†æ Streamlit åº”ç”¨
ä¼˜åŒ–ç‰ˆæœ¬ - ä½¿ç”¨ç»Ÿä¸€é…ç½®å’Œå·¥å…·å‡½æ•°
"""
import streamlit as st
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
import torch.nn.functional as F
import pandas as pd
import altair as alt
from pathlib import Path
import sys

# å¯¼å…¥é¡¹ç›®é…ç½®å’Œå·¥å…·
import config
import utils

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title=config.APP_TITLE,
    page_icon=config.APP_ICON,
    layout=config.APP_LAYOUT
)

# ============= æ¨¡å‹åŠ è½½ =============
@st.cache_resource
def load_model():
    """
    åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆå¸¦ç¼“å­˜ï¼‰

    Returns:
        tuple: (tokenizer, model, device) æˆ– (None, None, None) å¦‚æœå¤±è´¥
    """
    try:
        model_path = config.MODEL_DIR

        # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
        if not Path(model_path).exists():
            st.error(
                f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}\n\n"
                "è¯·å…ˆè¿è¡Œ 04_BERT_Finetune.ipynb è®­ç»ƒæ¨¡å‹"
            )
            return None, None, None

        # ä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ï¼ˆå•æ¡æ•°æ®æ¨ç†æ—¶ CPU å·²è¶³å¤Ÿå¿«ï¼‰
        device = torch.device("cpu")

        # åŠ è½½ tokenizer å’Œæ¨¡å‹
        tokenizer = DistilBertTokenizer.from_pretrained(model_path)
        model = DistilBertForSequenceClassification.from_pretrained(model_path)
        model.to(device)
        model.eval()

        st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return tokenizer, model, device

    except Exception as e:
        st.error(
            f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼\n\n"
            f"é”™è¯¯ä¿¡æ¯: {e}\n\n"
            f"è¯·æ£€æŸ¥:\n"
            f"1. æ¨¡å‹è·¯å¾„ '{model_path}' æ˜¯å¦å­˜åœ¨\n"
            f"2. æ˜¯å¦å·²è¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶"
        )
        return None, None, None


# ============= ä¾§è¾¹æ  =============
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    with st.sidebar:
        st.image("https://cdn-icons-png.flaticon.com/512/4712/4712109.png", width=100)
        st.title("ğŸ¤– AI èˆ†æƒ…åˆ†æåŠ©æ‰‹")
        st.markdown("---")

        st.markdown(f"""
        **é¡¹ç›®ä¿¡æ¯:**
        - **è¯¾ç¨‹:** {config.MODEL_INFO['course']}
        - **æ¨¡å‹:** {config.MODEL_INFO['model_name']}
        - **å‡†ç¡®ç‡:** {config.MODEL_INFO['accuracy']}

        **ä¸šåŠ¡ä»·å€¼:**

        è¯¥ç³»ç»Ÿå¸®åŠ©äº§å“ç»ç†ä»æµ·é‡è¯„è®ºä¸­ï¼š
        1. ğŸ” **è‡ªåŠ¨è¯†åˆ«** ç”¨æˆ·ç—›ç‚¹ (Negative)
        2. âš–ï¸ **ç²¾å‡†åˆ¤æ–­** ä¸­ç«‹å»ºè®® (Neutral)
        3. ğŸš€ **é‡åŒ–** ç”¨æˆ·æ»¡æ„åº¦

        **æŠ€æœ¯äº®ç‚¹:**
        - åŸºäº DistilBERT é¢„è®­ç»ƒæ¨¡å‹
        - éšæœºç§å­æ§åˆ¶ç¡®ä¿å¯å¤ç°
        - ç»Ÿä¸€é…ç½®ç®¡ç†
        - å®Œå–„çš„é”™è¯¯å¤„ç†
        """)

        st.markdown("---")
        st.info("ğŸ’¡ Tip: å°è¯•è¾“å…¥å¤æ‚å¥å­ï¼Œæµ‹è¯•æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚")

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        with st.expander("âš™ï¸ é…ç½®ä¿¡æ¯"):
            st.text(f"è®¾å¤‡: {utils.get_device_info()}")
            st.text(f"éšæœºç§å­: {config.RANDOM_SEED}")
            st.text(f"æœ€å¤§åºåˆ—é•¿åº¦: {config.MAX_SEQ_LENGTH}")


# ============= ä¸»ç•Œé¢ =============
def render_main_interface(tokenizer, model, device):
    """
    æ¸²æŸ“ä¸»ç•Œé¢

    Args:
        tokenizer: BERT tokenizer
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        device: è¿è¡Œè®¾å¤‡
    """
    st.title("Voice of User Analysis Dashboard")
    st.markdown("### ğŸ—£ï¸ å®æ—¶è¯„è®ºæƒ…æ„Ÿæ£€æµ‹ (Real-time Sentiment Detection)")

    # è¾“å…¥åŒº
    text_input = st.text_area(
        "è¯·è¾“å…¥ç”¨æˆ·è¯„è®º (Enter user review):",
        height=100,
        placeholder="ä¾‹å¦‚: I love the design but the price is too high.",
        max_chars=config.MAX_TEXT_LENGTH,
        help=f"æœ€å¤§é•¿åº¦: {config.MAX_TEXT_LENGTH} å­—ç¬¦"
    )

    col1, col2 = st.columns([1, 4])
    with col1:
        analyze_btn = st.button("ğŸš€ å¼€å§‹åˆ†æ", use_container_width=True)

    # ============= æ¨ç†é€»è¾‘ =============
    if analyze_btn:
        if not text_input or not text_input.strip():
            st.warning("âš ï¸ è¯·å…ˆè¾“å…¥ä¸€æ®µæ–‡å­—")
            return

        # è¾“å…¥éªŒè¯
        is_valid, cleaned_text, warning = utils.validate_input_text(text_input)

        if not is_valid:
            st.error(f"âŒ {warning}")
            return

        if warning:
            st.warning(f"âš ï¸ {warning}")
            text_input = cleaned_text

        # è¿›è¡Œæ¨ç†
        with st.spinner("æ­£åœ¨è°ƒç”¨ BERT æ¨¡å‹è¿›è¡Œåˆ†æ..."):
            try:
                # é¢„å¤„ç†
                inputs = tokenizer(
                    text_input,
                    return_tensors="pt",
                    truncation=True,
                    max_length=config.MAX_SEQ_LENGTH,
                    padding=True
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}

                # æ¨ç†
                with torch.no_grad():
                    outputs = model(**inputs)
                    logits = outputs.logits
                    probs = F.softmax(logits, dim=1)[0].cpu().numpy()

                # ç»“æœæ˜ å°„
                labels = ['Bad ğŸ˜¡', 'Neutral ğŸ˜', 'Good ğŸ˜']
                pred_idx = probs.argmax()
                pred_label = labels[pred_idx]
                confidence = probs[pred_idx]

                # ============= ç»“æœå±•ç¤º =============
                st.markdown("---")

                # å…³é”®è¯æå–
                keywords = utils.extract_keywords(text_input)
                if keywords:
                    kw_list = [k[0] for k in keywords]
                    st.info(f"ğŸ” **ç”¨æˆ·å…³æ³¨ç‚¹ (Key Topics):** {', '.join(kw_list)}")
                else:
                    st.info("ğŸ” **ç”¨æˆ·å…³æ³¨ç‚¹:** (å¥å­å¤ªçŸ­ï¼Œæ— æ³•æå–)")

                # ç¬¬ä¸€è¡Œï¼šå¤§å¡ç‰‡å±•ç¤ºç»“æœ
                r1, r2, r3 = st.columns(3)
                with r1:
                    st.metric("æƒ…æ„Ÿå€¾å‘ (Sentiment)", pred_label)
                with r2:
                    st.metric("ç½®ä¿¡åº¦ (Confidence)", f"{confidence:.2%}")
                with r3:
                    # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
                    if pred_idx == 0:
                        st.error("ğŸš¨ å»ºè®®ï¼šä¼˜å…ˆå¤„ç†ï¼è¿™æ˜¯è´Ÿé¢åé¦ˆï¼Œå¯èƒ½åŒ…å« Bug æˆ–æ ¸å¿ƒç—›ç‚¹ã€‚")
                    elif pred_idx == 1:
                        st.warning("âš ï¸ å»ºè®®ï¼šæŒç»­å…³æ³¨ã€‚ç”¨æˆ·å¯èƒ½æœ‰åŠŸèƒ½å»ºè®®æˆ–éƒ¨åˆ†ä¸æ»¡ã€‚")
                    else:
                        st.success("âœ… å»ºè®®ï¼šä¿æŒç°çŠ¶ã€‚ç”¨æˆ·å¯¹äº§å“éå¸¸æ»¡æ„ã€‚")

                # ç¬¬äºŒè¡Œï¼šå¯è§†åŒ–æ¦‚ç‡åˆ†å¸ƒ
                st.markdown("#### ğŸ“Š æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒ (Model Probability)")

                prob_df = pd.DataFrame({
                    'Sentiment': ['Bad', 'Neutral', 'Good'],
                    'Probability': probs
                })

                # ä½¿ç”¨ Altair ç”»æŸ±çŠ¶å›¾
                chart = alt.Chart(prob_df).mark_bar().encode(
                    x=alt.X('Sentiment', sort=None),
                    y=alt.Y('Probability', axis=alt.Axis(format='%')),
                    color=alt.Color(
                        'Sentiment',
                        scale=alt.Scale(
                            domain=['Bad', 'Neutral', 'Good'],
                            range=[
                                config.SENTIMENT_COLORS['Bad'],
                                config.SENTIMENT_COLORS['Neutral'],
                                config.SENTIMENT_COLORS['Good']
                            ]
                        ),
                        legend=None
                    ),
                    tooltip=['Sentiment', alt.Tooltip('Probability', format='.2%')]
                ).properties(height=300)

                st.altair_chart(chart, use_container_width=True)

                # è¯¦ç»†æ¦‚ç‡ä¿¡æ¯
                with st.expander("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ¦‚ç‡"):
                    for sent, prob in zip(['Bad', 'Neutral', 'Good'], probs):
                        st.text(f"{sent:10s}: {prob:.4f} ({prob*100:.2f}%)")

            except Exception as e:
                st.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
                st.exception(e)


# ============= æ‰¹é‡åˆ†æåŠŸèƒ½ =============
def render_batch_analysis(tokenizer, model, device):
    """
    æ¸²æŸ“æ‰¹é‡åˆ†æåŠŸèƒ½

    Args:
        tokenizer: BERT tokenizer
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        device: è¿è¡Œè®¾å¤‡
    """
    st.markdown("---")
    st.markdown("### ğŸ“ æ‰¹é‡åˆ†æ (Batch Analysis)")

    uploaded_file = st.file_uploader(
        "ä¸Šä¼  CSV æ–‡ä»¶ï¼ˆéœ€åŒ…å« 'text' åˆ—ï¼‰",
        type=['csv'],
        help="CSV æ–‡ä»¶å¿…é¡»åŒ…å«åä¸º 'text' çš„åˆ—"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            if 'text' not in df.columns:
                st.error("âŒ CSV æ–‡ä»¶å¿…é¡»åŒ…å« 'text' åˆ—")
                return

            st.info(f"ğŸ“Š ä¸Šä¼ äº† {len(df)} æ¡è®°å½•")

            if st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ"):
                with st.spinner(f"æ­£åœ¨åˆ†æ {len(df)} æ¡è®°å½•..."):
                    predictions = []
                    confidences = []

                    progress_bar = st.progress(0)

                    for idx, text in enumerate(df['text']):
                        # éªŒè¯è¾“å…¥
                        is_valid, cleaned_text, _ = utils.validate_input_text(str(text))
                        if not is_valid:
                            predictions.append('Error')
                            confidences.append(0.0)
                            continue

                        # æ¨ç†
                        inputs = tokenizer(
                            cleaned_text,
                            return_tensors="pt",
                            truncation=True,
                            max_length=config.MAX_SEQ_LENGTH,
                            padding=True
                        )
                        inputs = {k: v.to(device) for k, v in inputs.items()}

                        with torch.no_grad():
                            outputs = model(**inputs)
                            logits = outputs.logits
                            probs = F.softmax(logits, dim=1)[0].cpu().numpy()

                        pred_idx = probs.argmax()
                        predictions.append(list(config.LABEL_MAP.keys())[pred_idx])
                        confidences.append(probs[pred_idx])

                        # æ›´æ–°è¿›åº¦
                        progress_bar.progress((idx + 1) / len(df))

                    # æ·»åŠ ç»“æœåˆ° dataframe
                    df['predicted_sentiment'] = predictions
                    df['confidence'] = confidences

                    st.success("âœ… æ‰¹é‡åˆ†æå®Œæˆï¼")

                    # æ˜¾ç¤ºç»“æœ
                    st.dataframe(df)

                    # ç»Ÿè®¡ä¿¡æ¯
                    st.markdown("#### ğŸ“Š ç»Ÿè®¡ç»“æœ")
                    col1, col2, col3 = st.columns(3)

                    sentiment_counts = pd.Series(predictions).value_counts()
                    with col1:
                        st.metric("Bad", sentiment_counts.get('bad', 0))
                    with col2:
                        st.metric("Neutral", sentiment_counts.get('neutral', 0))
                    with col3:
                        st.metric("Good", sentiment_counts.get('good', 0))

                    # ä¸‹è½½ç»“æœ
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "ğŸ“¥ ä¸‹è½½ç»“æœ CSV",
                        csv,
                        "sentiment_analysis_results.csv",
                        "text/csv",
                        key='download-csv'
                    )

        except Exception as e:
            st.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")


# ============= ä¸»å‡½æ•° =============
def main():
    """ä¸»å‡½æ•°"""
    # æ¸²æŸ“ä¾§è¾¹æ 
    render_sidebar()

    # åŠ è½½æ¨¡å‹
    tokenizer, model, device = load_model()

    # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåœæ­¢è¿è¡Œ
    if tokenizer is None or model is None:
        st.stop()

    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2, tab3 = st.tabs(["ğŸ” å•æ¡åˆ†æ", "ğŸ“ æ‰¹é‡åˆ†æ", "ğŸ“– ä½¿ç”¨è¯´æ˜"])

    with tab1:
        render_main_interface(tokenizer, model, device)

    with tab2:
        render_batch_analysis(tokenizer, model, device)

    with tab3:
        st.markdown("""
        ## ğŸ“– ä½¿ç”¨è¯´æ˜

        ### å•æ¡åˆ†æ
        1. åœ¨æ–‡æœ¬æ¡†ä¸­è¾“å…¥ç”¨æˆ·è¯„è®º
        2. ç‚¹å‡»"å¼€å§‹åˆ†æ"æŒ‰é’®
        3. æŸ¥çœ‹æƒ…æ„Ÿåˆ†ç±»ç»“æœå’Œç½®ä¿¡åº¦
        4. æ ¹æ®å»ºè®®é‡‡å–ç›¸åº”è¡ŒåŠ¨

        ### æ‰¹é‡åˆ†æ
        1. å‡†å¤‡åŒ…å« 'text' åˆ—çš„ CSV æ–‡ä»¶
        2. ä¸Šä¼ æ–‡ä»¶
        3. ç‚¹å‡»"å¼€å§‹æ‰¹é‡åˆ†æ"
        4. ä¸‹è½½åˆ†æç»“æœ

        ### æƒ…æ„Ÿåˆ†ç±»è¯´æ˜
        - **Bad (è´Ÿé¢)**: ç”¨æˆ·ä¸æ»¡ã€æŠ•è¯‰ã€æ‰¹è¯„
        - **Neutral (ä¸­æ€§)**: å®¢è§‚é™ˆè¿°ã€åŠŸèƒ½å»ºè®®
        - **Good (æ­£é¢)**: ç”¨æˆ·èµæ‰¬ã€æ»¡æ„è¡¨è¾¾

        ### æ³¨æ„äº‹é¡¹
        - è¾“å…¥æ–‡æœ¬æœ€å¤§é•¿åº¦: {0} å­—ç¬¦
        - æ”¯æŒä¸­è‹±æ–‡æ··åˆè¾“å…¥
        - æ¨¡å‹åŸºäºè®­ç»ƒæ•°æ®ï¼Œå¯èƒ½å­˜åœ¨åå·®
        """.format(config.MAX_TEXT_LENGTH))

        st.markdown("---")
        st.markdown("""
        ### ğŸ› ï¸ æŠ€æœ¯æ ˆ
        - **æ¨¡å‹**: DistilBERT (Hugging Face)
        - **æ¡†æ¶**: PyTorch, Transformers
        - **å‰ç«¯**: Streamlit
        - **æ•°æ®å¤„ç†**: Pandas, NumPy
        - **å¯è§†åŒ–**: Altair

        ### ğŸ“ æ”¯æŒ
        å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æŸ¥çœ‹é¡¹ç›® READMEã€‚
        """)


if __name__ == '__main__':
    main()
